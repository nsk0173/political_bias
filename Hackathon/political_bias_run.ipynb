{"cells":[{"cell_type":"markdown","source":["Install packages"],"metadata":{"id":"ieD-BmQGH7iM"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":32408,"status":"ok","timestamp":1706916111357,"user":{"displayName":"Seunggyun Na","userId":"04990652273056116740"},"user_tz":-540},"id":"FhuS5FeKWGXm","outputId":"9e4bfd85-1136-4e6c-e48e-42981be57dd9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.1)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n","Collecting gluonnlp\n","  Downloading gluonnlp-0.10.0.tar.gz (344 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m344.5/344.5 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from gluonnlp) (1.23.5)\n","Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from gluonnlp) (3.0.8)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gluonnlp) (23.2)\n","Building wheels for collected packages: gluonnlp\n","  Building wheel for gluonnlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gluonnlp: filename=gluonnlp-0.10.0-cp310-cp310-linux_x86_64.whl size=661772 sha256=690b8bed82e8b8db2ac3deeb885397ace4e2b7ccf248eace539d41a3f7d9d2e7\n","  Stored in directory: /root/.cache/pip/wheels/1a/1e/0d/99f55911d90f2b95b9f7c176d5813ef3622894a4b30fde6bd3\n","Successfully built gluonnlp\n","Installing collected packages: gluonnlp\n","Successfully installed gluonnlp-0.10.0\n","Collecting mxnet\n","  Downloading mxnet-1.9.1-py3-none-manylinux2014_x86_64.whl (49.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 MB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.10/dist-packages (from mxnet) (1.23.5)\n","Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.10/dist-packages (from mxnet) (2.31.0)\n","Collecting graphviz<0.9.0,>=0.8.1 (from mxnet)\n","  Downloading graphviz-0.8.4-py2.py3-none-any.whl (16 kB)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet) (2023.11.17)\n","Installing collected packages: graphviz, mxnet\n","  Attempting uninstall: graphviz\n","    Found existing installation: graphviz 0.20.1\n","    Uninstalling graphviz-0.20.1:\n","      Successfully uninstalled graphviz-0.20.1\n","Successfully installed graphviz-0.8.4 mxnet-1.9.1\n"]}],"source":["!pip install transformers\n","!pip install gluonnlp\n","!pip install mxnet"]},{"cell_type":"markdown","source":["Import packages"],"metadata":{"id":"2h0t_WkOH-jg"}},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":15643,"status":"ok","timestamp":1706916137167,"user":{"displayName":"Seunggyun Na","userId":"04990652273056116740"},"user_tz":-540},"id":"Sz8OdHr1WG_A"},"outputs":[],"source":["import torch\n","from torch import nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","import gluonnlp as nlp\n","import numpy as np\n","from tqdm.notebook import tqdm\n","import pandas as pd\n","from transformers import AdamW\n","from transformers.optimization import get_cosine_schedule_with_warmup\n","import csv"]},{"cell_type":"markdown","source":["Choose device settings according to runtime environment"],"metadata":{"id":"e9C4onPbH2_7"}},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1706916138085,"user":{"displayName":"Seunggyun Na","userId":"04990652273056116740"},"user_tz":-540},"id":"oSU567roWHen"},"outputs":[],"source":["# device = torch.device(\"cuda:0\") # GPU\n","device = torch.device(\"cpu\") # CPU"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20766,"status":"ok","timestamp":1706904687491,"user":{"displayName":"Seunggyun Na","userId":"04990652273056116740"},"user_tz":-540},"id":"_XxFTisKWSpv","outputId":"a5de9d6c-9b36-4389-fc0d-4bed55580ac7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["from google.colab import drive\n","drive.mount(\"/content/gdrive\", force_remount=True)\n","gdrive_path = \"/content/gdrive/My Drive/Hackathon/\" # change according to your file path"]},{"cell_type":"markdown","metadata":{"id":"7KJI3VLwe0C_"},"source":["Load fine-tuned model"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":406},"executionInfo":{"elapsed":318,"status":"error","timestamp":1706916262470,"user":{"displayName":"Seunggyun Na","userId":"04990652273056116740"},"user_tz":-540},"id":"-I3c3l8LV_Vl","outputId":"2506be95-b0fc-4832-fcb4-eb1d7614ca3c"},"outputs":[{"output_type":"error","ename":"HFValidationError","evalue":"Repo id must be in the form 'repo_name' or 'namespace/repo_name': 'Hackathon/model_save/'. Use `repo_type` argument if needed.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-c5bbd37bf4c9>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Load a trained model and vocabulary that you have fine-tuned\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0moutput_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Hackathon/model_save/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertForSequenceClassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2598\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2599\u001b[0m                 \u001b[0;31m# We make a call to the config file first (which may be absent) to get the commit hash as soon as possible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2600\u001b[0;31m                 resolved_config_file = cached_file(\n\u001b[0m\u001b[1;32m   2601\u001b[0m                     \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2602\u001b[0m                     \u001b[0mCONFIG_NAME\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m         \u001b[0;31m# Load from URL or cache if already cached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m         resolved_file = hf_hub_download(\n\u001b[0m\u001b[1;32m    431\u001b[0m             \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    108\u001b[0m         ):\n\u001b[1;32m    109\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0marg_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"repo_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"from_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"to_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m                 \u001b[0mvalidate_repo_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0marg_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"token\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marg_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrepo_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         raise HFValidationError(\n\u001b[0m\u001b[1;32m    159\u001b[0m             \u001b[0;34m\"Repo id must be in the form 'repo_name' or 'namespace/repo_name':\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;34mf\" '{repo_id}'. Use `repo_type` argument if needed.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': 'Hackathon/model_save/'. Use `repo_type` argument if needed."]}],"source":["import tensorflow as tf\n","from transformers import BertTokenizer, BertModel, BertForSequenceClassification\n","\n","# Load a trained model and vocabulary that you have fine-tuned\n","output_dir = gdrive_path + \"model_save/\"\n","model = BertForSequenceClassification.from_pretrained(output_dir)\n","tokenizer = BertTokenizer.from_pretrained(output_dir)\n","\n","# Copy the model to the GPU.\n","model.to(device)\n","model.eval()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vBfN7biEWcDA"},"outputs":[],"source":["# setup variables\n","batch_size = 32\n","max_len = 128"]},{"cell_type":"markdown","metadata":{"id":"5TbFexFlYigw"},"source":["Political bias evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9jKqPw6IW5Jx"},"outputs":[],"source":["from torch.utils.data import TensorDataset, DataLoader, SequentialSampler\n","\n","def evaluate(texts):\n","  # Tokenize all of the sentences and map the tokens to thier word IDs.\n","  input_ids = []\n","  attention_masks = []\n","\n","  # For every sentence...\n","  for text in texts:\n","    # `encode_plus` will:\n","    #   (1) Tokenize the sentence.\n","    #   (2) Prepend the `[CLS]` token to the start.\n","    #   (3) Append the `[SEP]` token to the end.\n","    #   (4) Map tokens to their IDs.\n","    #   (5) Pad or truncate the sentence to `max_length`\n","    #   (6) Create attention masks for [PAD] tokens.\n","    encoded_dict = tokenizer.encode_plus(\n","                        text,                      # Sentence to encode.\n","                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","                        max_length = max_len,           # Pad & truncate all sentences.\n","                        pad_to_max_length = True,\n","                        return_attention_mask = True,   # Construct attn. masks.\n","                        return_tensors = 'pt',     # Return pytorch tensors.\n","                        truncation = True\n","                   )\n","    # Add the encoded sentence to the list.\n","    input_ids.append(encoded_dict['input_ids'])\n","\n","    # And its attention mask (simply differentiates padding from non-padding).\n","    attention_masks.append(encoded_dict['attention_mask'])\n","\n","  # Convert the lists into tensors.\n","  input_ids = torch.cat(input_ids, dim=0)\n","  attention_masks = torch.cat(attention_masks, dim=0)\n","\n","  prediction_data = TensorDataset(input_ids, attention_masks)\n","  prediction_sampler = SequentialSampler(prediction_data)\n","  prediction_loader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)\n","\n","\n","  # evaluate\n","\n","  eval = []\n","\n","  # Predict\n","  for batch in prediction_loader:\n","    print(\"processing new batch\")\n","    # Add batch to GPU\n","    batch = tuple(t.to(device) for t in batch)\n","\n","    # Unpack the inputs from our dataloader\n","    b_input_ids, b_input_mask = batch\n","\n","    # Telling the model not to compute or store gradients, saving memory and\n","    # speeding up prediction\n","    with torch.no_grad():\n","      # Forward pass, calculate logit predictions\n","      outputs = model(b_input_ids, token_type_ids=None,\n","                      attention_mask=b_input_mask)\n","\n","    logits = outputs[0]\n","\n","    # Move logits and labels to CPU\n","    logits = logits.detach().cpu().numpy()\n","\n","    # Calculate evaluations\n","    sigmoid = lambda x:1 / (1 + np.exp(-x))\n","    eval += [sigmoid(val[1] - val[0]) for val in list(logits)]\n","    print(\"finished batch\")\n","  return eval\n"]},{"cell_type":"markdown","metadata":{"id":"eWUYMaZIYsOt"},"source":["Evaluate all the articles from csv file and save the data as csv file."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vmzmqJGWYvA2"},"outputs":[],"source":["def evaluate_csv(csv_path, out_dir, out_name, max_len = None, encoding = 'UTF-8', mode = None):\n","  dataset = pd.read_csv(csv_path, names=['time',\t'category_name',\t'text_company',\t'text_headline',\t'text_sentence',\t'content_url'], encoding = encoding).drop(0)\n","  if max_len: dataset = dataset.sample(max_len)\n","  titles = dataset.text_headline.values\n","  texts = dataset.text_sentence.values\n","  print(len(titles))\n","\n","  eval = evaluate(texts)\n","\n","  list = dataset.values.tolist()\n","  pair_data, notext_data = [], []\n","  print(eval)\n","  for i in range(len(eval)):\n","    pair_data.append([list[i][3], list[i][4], eval[i].item()])\n","    notext_data.append(list[i][:4]+list[i][5:]+[eval[i].item()])\n","\n","  pair_data.sort(key = lambda x: x[2])\n","  pair_data = [['text_headline', 'text_sentence', 'bias_label']] + pair_data\n","  notext_data = [['time',\t'category_name',\t'text_company',\t'text_headline',\t'content_url', \"bias_label\"]] + notext_data\n","\n","  csv_path_1 = out_dir + out_name + \"_pair_sorted.csv\"\n","  csv_path_2 = out_dir + out_name + \"_notext.csv\"\n","  with open(csv_path_1, 'w', newline='') as csvfile:\n","    csv_writer = csv.writer(csvfile)\n","    csv_writer.writerows(pair_data)\n","  csvfile.close()\n","  if not mode == \"pair\":\n","    with open(csv_path_2, 'w', newline='') as csvfile:\n","      csv_writer = csv.writer(csvfile)\n","      csv_writer.writerows(notext_data)\n","    csvfile.close()\n","\n","  print(\"pair data at \"+csv_path_1)\n","  print(\"notext data at \"+csv_path_2)\n","\n","  return"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OWlkiS1lhGNG","executionInfo":{"status":"ok","timestamp":1706895598853,"user_tz":-540,"elapsed":168004,"user":{"displayName":"Seunggyun Na","userId":"04990652273056116740"}},"outputId":"b62939a7-475b-47aa-9114-0451639e4d18"},"outputs":[{"output_type":"stream","name":"stdout","text":["349\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["processing new batch\n","finished batch\n","processing new batch\n","finished batch\n","processing new batch\n","finished batch\n","processing new batch\n","finished batch\n","processing new batch\n","finished batch\n","processing new batch\n","finished batch\n","processing new batch\n","finished batch\n","processing new batch\n","finished batch\n","processing new batch\n","finished batch\n","processing new batch\n","finished batch\n","processing new batch\n","finished batch\n","[0.9997192649091218, 0.9993543237144058, 0.0002966254509851064, 0.0002763185491441242, 0.030206482714213976, 0.9998016827194881, 0.0003012854650358806, 0.9991854137048427, 0.9994011092115678, 0.9997633144169369, 0.9231334253900888, 0.9998588053157546, 0.009147887332299143, 0.00025552823100350976, 0.9997608203727333, 0.01056274125829042, 0.00046869809606274145, 0.00048808551792117516, 0.9962173360596647, 0.999741978073166, 0.01053559674399003, 0.05779041561000817, 0.011263312409975221, 0.999794051521481, 0.000230246652851118, 0.008908945737824065, 0.0002779138359905338, 0.04294639490634105, 0.00033989898792918783, 0.7223613579839794, 0.00032181003057352425, 0.0002852113460638606, 0.00020969185945075475, 0.9991607252367293, 0.0034468800938466963, 0.001518728028198063, 0.0002324455645515974, 0.0002347400351536957, 0.9998316741751742, 0.9998348495371613, 0.999678686625501, 0.9986391056475534, 0.9988592772748367, 0.026781607668817896, 0.9998610797540471, 0.9997843080160693, 0.014321310586976842, 0.012837621502310825, 0.22021343509109792, 0.07260216950162725, 0.0008364903389552387, 0.9998359933922066, 0.999808736543591, 0.9998334692075244, 0.9997654937468058, 0.9998358696396815, 0.00021094840776698028, 0.999853696283249, 0.9996025891676381, 0.00021031479030547402, 0.04504892555896359, 0.006423800683391393, 0.9998248372986337, 0.1826198396109687, 0.9997488414803952, 0.9997568207006442, 0.999846022060012, 0.9998431412399128, 0.9950247600116722, 0.9998040816607678, 0.9961522397861375, 0.9998193517673899, 0.19089807164139325, 0.10336269633176262, 0.9998256331556674, 0.00020852908390159546, 0.9996558233854446, 0.9997722120240325, 0.9998275110254365, 0.9998036586570491, 0.9998096295156771, 0.0002249257733965088, 0.9995136963474429, 0.0005363610144372907, 0.9998082989168651, 0.9997936875390085, 0.8506424747338683, 0.0005448807614389375, 0.7732635572242118, 0.003158631794220074, 0.0011460115524455567, 0.9998187890302361, 0.9997884606239992, 0.0002767414424836896, 0.00040699640330839587, 0.9998501664666324, 0.996576755749082, 0.00032838602996442353, 0.9998316929553731, 0.9646928237253994, 0.9996316829600729, 0.9998417494822904, 0.9995777626868573, 0.9998427082382159, 0.9998485669655381, 0.9675580274671464, 0.0970295561976017, 0.9998426773396712, 0.999767421847787, 0.9995316886969686, 0.9998618778218682, 0.9998618343382101, 0.00021470811385946117, 0.0036039646690919666, 0.9998126625750343, 0.00022613642386445947, 0.9997331705234402, 0.969771577464036, 0.0002806644209401696, 0.00034923926632810805, 0.00020983984957105784, 0.00020718180344880818, 0.9996707088705346, 0.9998449682595348, 0.00023815833831531582, 0.00020821796028344075, 0.9996663677411721, 0.13235447348731916, 0.9998519495054168, 0.999848512237815, 0.9997919904122268, 0.9994415524863869, 0.989949292599483, 0.9998381922703519, 0.99970881600591, 0.9997974206156398, 0.9932167180017855, 0.9998539187606253, 0.9996643650506227, 0.9749203593638195, 0.9998286713890023, 0.9961102780271154, 0.0007851409769887179, 0.00037941510049868633, 0.9998044620153113, 0.9995816542337362, 0.9997985136193228, 0.9998436289691675, 0.9998251515099963, 0.0007304925626101769, 0.9996229301372129, 0.9997646852149413, 0.9998590856084154, 0.9998348446638797, 0.9994672210400541, 0.9942065635469897, 0.015474700779201338, 0.0002072144000247686, 0.0007467839837078572, 0.001499310055654016, 0.0006106095041160611, 0.9997488479382005, 0.00036801874020428633, 0.9998586920465482, 0.9998560476557625, 0.9992058589860403, 0.0002581287067795904, 0.9998513478895033, 0.9995575258230144, 0.00033019050444338394, 0.5346931964393595, 0.0005884057350596874, 0.0002825142053743086, 0.0028756585699548616, 0.011327175195809277, 0.0004384457867564967, 0.6846129637142866, 0.9967732581712456, 0.9998387702893845, 0.9998184145561773, 0.003277034307953019, 0.9998579995134179, 0.9997606816866065, 0.9997160771337957, 0.0022642423629325946, 0.00026322728345651867, 0.001255313956824831, 0.9937344429775137, 0.9972990356708756, 0.0002988466750936176, 0.0014417251846880407, 0.9998343994934492, 0.0020070365102652678, 0.9994679084384461, 0.9585626772818817, 0.9994357712781412, 0.9996706037003921, 0.999863647300092, 0.006785177427068091, 0.9981925553749875, 0.0011600720258586172, 0.9998451430320718, 0.00046108056752533917, 0.24462124510421165, 0.999845557109683, 0.9998556815332772, 0.321585453184876, 0.9997134564325413, 0.0003014498146407463, 0.9998505474381433, 0.00032442009709633717, 0.00022605255721955354, 0.000856819517746163, 0.9998568436352224, 0.0250583170041898, 0.9998485281236925, 0.9813704626677783, 0.9740278942994134, 0.005202204173654364, 0.00027367780745590287, 0.9904285702332618, 0.00021522362449194403, 0.9997528345144258, 0.9997528345144258, 0.9997528345144258, 0.00021834238046927406, 0.9997528345144258, 0.9997528345144258, 0.9997658409382485, 0.9998397463071838, 0.9998506586979398, 0.0008323002393066386, 0.00046083358900821884, 0.9995102684704267, 0.9995991982819641, 0.00023842095478046239, 0.0003150528094195867, 0.005246764704587277, 0.9998545205251095, 0.9993850103716422, 0.0016274359639111308, 0.9994801513470981, 0.9998027681684786, 0.0006414821244021147, 0.15199185492867265, 0.0005441481719723439, 0.9997500263727104, 0.9998237301415593, 0.00036408795035106133, 0.9987677992726965, 0.03990609864087814, 0.03990609864087814, 0.00021030155168247916, 0.00021030155168247916, 0.000278242836707458, 0.00021030155168247916, 0.00031876416626944366, 0.00031876416626944366, 0.00031876416626944366, 0.00031876416626944366, 0.00031876416626944366, 0.0005027491640076767, 0.00031876416626944366, 0.0002657560714816355, 0.00021107513718337956, 0.00991111962713526, 0.00021107513718337956, 0.001134302950966944, 0.00991111962713526, 0.00040131288884534365, 0.00991111962713526, 0.8843040347505875, 0.999757456106869, 0.9998559295718848, 0.9998545166408785, 0.999497112346933, 0.9998217921061704, 0.9986223657756614, 0.9998490299111085, 0.0005025613327887283, 0.9998364071276548, 0.9995070829291304, 0.9921894712119238, 0.9998522404721834, 0.9998432339356381, 0.998915395105027, 0.00031125348526394084, 0.9995547574229242, 0.9998240337621596, 0.0003015371844451278, 0.0002541627054577532, 0.9998573604733728, 0.9997180137994894, 0.9998303048809132, 0.9998471440903256, 0.0002541627054577532, 0.8526452167940841, 0.0005315666701208984, 0.00047011806321917246, 0.00020859688041735671, 0.00023713713415836522, 0.00025967617062854566, 0.05587536577461197, 0.0002541627054577532, 0.0002541627054577532, 0.0002541627054577532, 0.0002541627054577532, 0.0002541627054577532, 0.00022723913768073168, 0.0007556871407268968, 0.9990072175626316, 0.00035356311079947993, 0.0007877023811939134, 0.9997619783270334, 0.9997445872301949, 0.00023037446874950084, 0.004911600778949186, 0.7945586151585691, 0.9997564619959624, 0.0002384252848151849, 0.9998310903891873, 0.9977123610105741, 0.9977123610105741, 0.9977123610105741, 0.21003253816190884, 0.005027543089801742, 0.0007696416215274173, 0.9994644211602302, 0.00024341545158023338, 0.0002398734292858284, 0.0002594457804874604, 0.0028322441593607536, 0.00043940247271166894, 0.9998121397172236, 0.025426572894798407, 0.00029916906813874986, 0.9998093283330683, 0.9996964963790151, 0.9998016719407955, 0.9998411079317884, 0.9997883181919446, 0.999678011385566, 0.9998516064729682, 0.9998204511468348, 0.0003226456184512333, 0.9998381628122598, 0.9998323565325529, 0.9998476224403603, 0.986590419107472]\n","pair data at /content/gdrive/My Drive/Hackathon/output/생활문화_pair_sorted.csv\n","notext data at /content/gdrive/My Drive/Hackathon/output/생활문화_notext.csv\n"]}],"source":["import csv\n","\n","dataset_path = gdrive_path + \"dataset/articles/Article_정치_20240101_20240201.csv\"\n","\n","evaluate_csv(dataset_path, gdrive_path + \"output/\", \"정치\")\n"]},{"cell_type":"markdown","source":["merge all pair_sorted files"],"metadata":{"id":"yalgjlPA-7QS"}},{"cell_type":"code","source":["all_data = []\n","\n","file_names = ['특검', '태영', '총선_2024', '총선_2020', '총선_2016', '정치', '생활문화', '북한', '금리']\n","for name in file_names:\n","  csv_path = gdrive_path + \"output/done/\" + name + \"_pair_sorted.csv\"\n","  encoding = 'utf-8'\n","  # if name == '생활문화': encoding = 'cp949'\n","  dataset = pd.read_csv(csv_path, names=['text_headline',\t'text_sentence',\t'bias_label'], encoding = encoding).drop(0)\n","  all_data += dataset.values.tolist()\n","\n","all_data.sort(key = lambda x: x[2])\n","all_data = [['text_headline', 'text_sentence', 'bias_label']] + all_data\n","csv_path = gdrive_path + \"output/all_pairs.csv\"\n","with open(csv_path, 'w', newline='') as csvfile:\n","  csv_writer = csv.writer(csvfile)\n","  csv_writer.writerows(all_data)\n","csvfile.close()"],"metadata":{"id":"2Lwa2uUM-97O"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["merge all notext files except 생활문화, because of encoding issue"],"metadata":{"id":"TJgFy_ehb94v"}},{"cell_type":"code","source":["import pandas as pd\n","import csv\n","\n","all_data = []\n","\n","file_names = ['특검', '태영', '총선', '총선_2020', '총선_2016', '정치', '북한', '금리']\n","\n","for name in file_names:\n","  csv_path = gdrive_path + \"output/done/\" + name + \"_notext.csv\"\n","  encoding = 'utf-8'\n","  # if name == '생활문화': encoding = 'cp949'\n","  dataset = pd.read_csv(csv_path, names=['time',\t'category_name',\t'text_company',\t'text_headline',\t'content_url', \"bias_label\"], encoding = encoding).drop(0)\n","  dataset = dataset.sample(min(40, len(dataset)))\n","  all_data += dataset.values.tolist()\n","\n","all_data = [['time',\t'category_name',\t'text_company',\t'text_headline',\t'content_url', \"bias_label\"]] + all_data\n","csv_path = gdrive_path + \"output/all_notext.csv\"\n","with open(csv_path, 'w', newline='') as csvfile:\n","  csv_writer = csv.writer(csvfile)\n","  csv_writer.writerows(all_data)\n","csvfile.close()"],"metadata":{"id":"sqPdQjmNcA2L"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"authorship_tag":"ABX9TyPF29ti13AVUrMG6KCA7mNV"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}